{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550c9f00",
   "metadata": {},
   "source": [
    "# Full Model Training + Export Notebook\n",
    "This notebook contains all required cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ac1acd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 1 - Install XGBoost (optional)\n",
    "!conda install -y -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e89058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Configuration\n",
    "BUCKET_NAME = \"cloud-project-cloutserfers\"\n",
    "HOUSING_KEY = \"part-0.parquet\"\n",
    "ELECTRICITY_KEY = \"electricity_all_cleaned.parquet\"\n",
    "MODEL_DIR = \"backend/app/models\"\n",
    "RANDOM_SEED = 123\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "datasets = [\n",
    "    {\"name\": \"housing\", \"s3_key\": HOUSING_KEY, \"target\": \"price\"},\n",
    "    {\"name\": \"electricity\", \"s3_key\": ELECTRICITY_KEY, \"target\": \"demand_mw\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47e7b05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK; model dir: backend/app/models\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.api.types import is_string_dtype, is_bool_dtype\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# Make sure model dir exists\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Imports OK; model dir:\", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc2653e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean_nullable_dtypes() is ready.\n"
     ]
    }
   ],
   "source": [
    "def clean_nullable_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert pandas nullable dtypes into normal dtypes and replace pd.NA with np.nan.\n",
    "    This helps avoid weird dtype issues when training.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert pandas StringDtype to object\n",
    "    for col in df.columns:\n",
    "        if is_string_dtype(df[col]):\n",
    "            df[col] = df[col].astype(\"object\")\n",
    "    \n",
    "    # Convert nullable bool to float\n",
    "    for col in df.columns:\n",
    "        if is_bool_dtype(df[col]):\n",
    "            df[col] = df[col].astype(\"float64\")\n",
    "    \n",
    "    # Replace pandas NA with numpy nan\n",
    "    df = df.replace({pd.NA: np.nan})\n",
    "    return df\n",
    "\n",
    "print(\"clean_nullable_dtypes() is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67ed7634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "Dataset: housing\n",
      "S3 path: s3://cloud-project-cloutserfers/part-0.parquet\n",
      "Target : price\n",
      "==============================\n",
      "\n",
      "Downloading parquet from S3...\n",
      "Downloaded to: /tmp/housing.parquet\n",
      "Reading parquet into pandas...\n",
      "Raw shape: (100420, 15)\n",
      "Raw columns: ['transaction_unique_identifier', 'price', 'date_of_transfer', 'property_type', 'oldnew', 'duration', 'towncity', 'district', 'county', 'ppdcategory_type', 'record_status__monthly_file_only', 'year', 'month', 'region', 'is_new_build']\n",
      "Housing feature columns: ['region', 'property_type', 'tenure', 'year', 'month', 'is_new_build']\n",
      "    region property_type tenure    year  month  is_new_build     price\n",
      "0  NORFOLK             T      F  2016.0    3.0           0.0  146500.0\n",
      "1  NORFOLK             S      F  2016.0    3.0           0.0  150000.0\n",
      "2  NORFOLK             T      F  2016.0    3.0           0.0  425000.0\n",
      "3   SURREY             D      F  2016.0    3.0           0.0  460000.0\n",
      "4   SURREY             S      F  2016.0    3.0           0.0  400000.0\n",
      "Final X shape: (100420, 6)\n",
      "Final y length: 100420\n",
      "Train shape: (80336, 6)\n",
      "Test shape : (20083, 6)\n",
      "[housing] Linear Regression - RMSE: 107927.898, MAE: 78010.956, R2: 0.490\n",
      "Saved LinearRegression bundle to: backend/app/models/housing_linear.joblib\n",
      "Starting XGBoost hyperparameter search for housing...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best params: {'model__subsample': 0.7, 'model__reg_lambda': 2.0, 'model__n_estimators': 600, 'model__max_depth': 4, 'model__learning_rate': 0.1, 'model__colsample_bytree': 0.85}\n",
      "[housing] XGBRegressor - RMSE: 104415.872, MAE: 74723.322, R2: 0.523\n",
      "Saved XGBRegressor bundle to: backend/app/models/housing_xgb.joblib\n",
      "\n",
      "==============================\n",
      "Dataset: electricity\n",
      "S3 path: s3://cloud-project-cloutserfers/electricity_all_cleaned.parquet\n",
      "Target : demand_mw\n",
      "==============================\n",
      "\n",
      "Downloading parquet from S3...\n",
      "Downloaded to: /tmp/electricity.parquet\n",
      "Reading parquet into pandas...\n",
      "Raw shape: (434928, 2)\n",
      "Raw columns: ['ts', 'demand_mw']\n",
      "Electricity feature columns: ['year', 'month', 'day', 'hour', 'is_weekend']\n",
      "   year  month  day  hour  is_weekend  demand_mw\n",
      "0  2001      1    1     0           0    38631.0\n",
      "1  2001      1    1     0           0    39808.0\n",
      "2  2001      1    1     1           0    40039.0\n",
      "3  2001      1    1     1           0    39339.0\n",
      "4  2001      1    1     2           0    38295.0\n",
      "Final X shape: (434928, 5)\n",
      "Final y length: 434928\n",
      "Train shape: (347942, 5)\n",
      "Test shape : (86986, 5)\n",
      "[electricity] Linear Regression - RMSE: 6053.179, MAE: 4916.777, R2: 0.495\n",
      "Saved LinearRegression bundle to: backend/app/models/electricity_linear.joblib\n",
      "Starting XGBoost hyperparameter search for electricity...\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Best params: {'model__subsample': 0.85, 'model__reg_lambda': 1.0, 'model__n_estimators': 600, 'model__max_depth': 8, 'model__learning_rate': 0.1, 'model__colsample_bytree': 1.0}\n",
      "[electricity] XGBRegressor - RMSE: 1065.067, MAE: 814.119, R2: 0.984\n",
      "Saved XGBRegressor bundle to: backend/app/models/electricity_xgb.joblib\n",
      "\n",
      "Training finished for all datasets. All 4 models saved in: backend/app/models\n"
     ]
    }
   ],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "results = []  # to store metrics for later display\n",
    "\n",
    "for ds in datasets:\n",
    "    name   = ds[\"name\"]\n",
    "    s3_key = ds[\"s3_key\"]\n",
    "    target = ds[\"target\"]\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"S3 path: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "    print(f\"Target : {target}\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    # 1) Download parquet from S3\n",
    "    local_parquet = f\"/tmp/{name}.parquet\"\n",
    "    print(\"Downloading parquet from S3...\")\n",
    "    s3.download_file(BUCKET_NAME, s3_key, local_parquet)\n",
    "    print(\"Downloaded to:\", local_parquet)\n",
    "\n",
    "    # 2) Read parquet\n",
    "    print(\"Reading parquet into pandas...\")\n",
    "    df = pd.read_parquet(local_parquet)\n",
    "    print(\"Raw shape:\", df.shape)\n",
    "    print(\"Raw columns:\", list(df.columns))\n",
    "\n",
    "    # 3) Clean nullable dtypes\n",
    "    df = clean_nullable_dtypes(df)\n",
    "\n",
    "    # 4) Dataset-specific feature engineering and fixed feature list\n",
    "    if name == \"housing\":\n",
    "        # We know housing has region/property_type/duration/year/month/is_new_build, etc.\n",
    "        # Rename duration -> tenure\n",
    "        if \"tenure\" not in df.columns and \"duration\" in df.columns:\n",
    "            df = df.rename(columns={\"duration\": \"tenure\"})\n",
    "\n",
    "        # Required raw input columns (as you specified)\n",
    "        feature_cols = [\"region\", \"property_type\", \"tenure\", \"year\", \"month\", \"is_new_build\"]\n",
    "        missing = [c for c in feature_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Housing dataset is missing required columns: {missing}\")\n",
    "\n",
    "        # Define which are categorical vs numeric\n",
    "        cat_cols = [\"region\", \"property_type\", \"tenure\", \"is_new_build\"]\n",
    "        num_cols = [\"year\", \"month\"]\n",
    "\n",
    "        # Do NOT force categoricals to numeric; leave them as strings/ints\n",
    "        # Just make sure numeric ones are numeric:\n",
    "        for col in num_cols:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "        print(\"Housing feature columns:\", feature_cols)\n",
    "        print(df[feature_cols + [target]].head())\n",
    "\n",
    "    elif name == \"electricity\":\n",
    "        # We know electricity has 'ts' and 'demand_mw'\n",
    "        ts_candidates = [\"ts\", \"timestamp\", \"datetime\"]\n",
    "        ts_col = None\n",
    "        for c in ts_candidates:\n",
    "            if c in df.columns:\n",
    "                ts_col = c\n",
    "                break\n",
    "        if ts_col is None:\n",
    "            raise ValueError(\n",
    "                f\"Electricity dataset: expected one of {ts_candidates}, found {list(df.columns)}\"\n",
    "            )\n",
    "\n",
    "        df[ts_col] = pd.to_datetime(df[ts_col])\n",
    "\n",
    "        # Required raw inputs for electricity:\n",
    "        # year, month, day, hour, is_weekend\n",
    "        df[\"year\"]       = df[ts_col].dt.year\n",
    "        df[\"month\"]      = df[ts_col].dt.month\n",
    "        df[\"day\"]        = df[ts_col].dt.day\n",
    "        df[\"hour\"]       = df[ts_col].dt.hour\n",
    "        df[\"is_weekend\"] = (df[ts_col].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "        feature_cols = [\"year\", \"month\", \"day\", \"hour\", \"is_weekend\"]\n",
    "        cat_cols = []                      # all numeric here\n",
    "        num_cols = feature_cols[:]         # all numeric\n",
    "\n",
    "        for col in num_cols:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "        print(\"Electricity feature columns:\", feature_cols)\n",
    "        print(df[feature_cols + [target]].head())\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset name: {name}\")\n",
    "\n",
    "    # 5) Ensure target exists and numeric\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Target '{target}' not in columns: {list(df.columns)}\")\n",
    "    df[target] = pd.to_numeric(df[target])\n",
    "\n",
    "    # 6) Build final X, y (with fixed raw feature columns)\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target].copy()\n",
    "\n",
    "    print(\"Final X shape:\", X.shape)\n",
    "    print(\"Final y length:\", len(y))\n",
    "\n",
    "    # 7) Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # Drop rows with NaN\n",
    "    train_mask = ~X_train.isna().any(axis=1)\n",
    "    test_mask  = ~X_test.isna().any(axis=1)\n",
    "    X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "    X_test, y_test   = X_test[test_mask], y_test[test_mask]\n",
    "\n",
    "    print(\"Train shape:\", X_train.shape)\n",
    "    print(\"Test shape :\", X_test.shape)\n",
    "\n",
    "    # 8) Preprocessor: OneHot for categoricals, passthrough numerics\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---------- 9) Linear Regression pipeline ----------\n",
    "    lin_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", LinearRegression()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    lin_pipeline.fit(X_train, y_train)\n",
    "    y_pred_lin = lin_pipeline.predict(X_test)\n",
    "\n",
    "    mse_lin  = mean_squared_error(y_test, y_pred_lin)\n",
    "    rmse_lin = mse_lin ** 0.5\n",
    "    mae_lin  = mean_absolute_error(y_test, y_pred_lin)\n",
    "    r2_lin   = r2_score(y_test, y_pred_lin)\n",
    "\n",
    "    print(f\"[{name}] Linear Regression - RMSE: {rmse_lin:.3f}, MAE: {mae_lin:.3f}, R2: {r2_lin:.3f}\")\n",
    "\n",
    "    lin_bundle = {\n",
    "        \"model\": lin_pipeline,        # pipeline with preprocessing + model\n",
    "        \"features\": feature_cols,     # RAW inputs backend must provide\n",
    "        \"target\": target,\n",
    "        \"model_type\": \"linear_regression\",\n",
    "    }\n",
    "\n",
    "    lin_filename = os.path.join(MODEL_DIR, f\"{name}_linear.joblib\")\n",
    "    joblib.dump(lin_bundle, lin_filename)\n",
    "    print(f\"Saved LinearRegression bundle to: {lin_filename}\")\n",
    "\n",
    "    results.append({\n",
    "        \"dataset\": name,\n",
    "        \"model_type\": \"linear_regression\",\n",
    "        \"rmse\": rmse_lin,\n",
    "        \"mae\": mae_lin,\n",
    "        \"r2\": r2_lin,\n",
    "        \"file\": lin_filename,\n",
    "    })\n",
    "\n",
    "    # ---------- 10) XGBoost pipeline ----------\n",
    "    xgb = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=RANDOM_SEED,\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"rmse\",\n",
    "    )\n",
    "\n",
    "    xgb_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", xgb),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    param_dist = {\n",
    "        \"model__n_estimators\": [200, 400, 600],\n",
    "        \"model__max_depth\": [4, 6, 8],\n",
    "        \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"model__subsample\": [0.7, 0.85, 1.0],\n",
    "        \"model__colsample_bytree\": [0.7, 0.85, 1.0],\n",
    "        \"model__reg_lambda\": [0.5, 1.0, 2.0],\n",
    "    }\n",
    "\n",
    "    tuner = RandomizedSearchCV(\n",
    "        estimator=xgb_pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    print(f\"Starting XGBoost hyperparameter search for {name}...\")\n",
    "    tuner.fit(X_train, y_train)\n",
    "    print(\"Best params:\", tuner.best_params_)\n",
    "\n",
    "    best_xgb_pipeline = tuner.best_estimator_\n",
    "    y_pred_xgb = best_xgb_pipeline.predict(X_test)\n",
    "\n",
    "    mse_xgb  = mean_squared_error(y_test, y_pred_xgb)\n",
    "    rmse_xgb = mse_xgb ** 0.5\n",
    "    mae_xgb  = mean_absolute_error(y_test, y_pred_xgb)\n",
    "    r2_xgb   = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "    print(f\"[{name}] XGBRegressor - RMSE: {rmse_xgb:.3f}, MAE: {mae_xgb:.3f}, R2: {r2_xgb:.3f}\")\n",
    "\n",
    "    xgb_bundle = {\n",
    "        \"model\": best_xgb_pipeline,   # pipeline (preprocessor + XGB)\n",
    "        \"features\": feature_cols,     # RAW inputs backend must provide\n",
    "        \"target\": target,\n",
    "        \"model_type\": \"xgboost\",\n",
    "    }\n",
    "\n",
    "    xgb_filename = os.path.join(MODEL_DIR, f\"{name}_xgb.joblib\")\n",
    "    joblib.dump(xgb_bundle, xgb_filename)\n",
    "    print(f\"Saved XGBRegressor bundle to: {xgb_filename}\")\n",
    "\n",
    "    results.append({\n",
    "        \"dataset\": name,\n",
    "        \"model_type\": \"xgboost\",\n",
    "        \"rmse\": rmse_xgb,\n",
    "        \"mae\": mae_xgb,\n",
    "        \"r2\": r2_xgb,\n",
    "        \"file\": xgb_filename,\n",
    "    })\n",
    "\n",
    "print(\"\\nTraining finished for all datasets. All 4 models saved in:\", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "980ccd21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model_type</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>r2</th>\n",
       "      <th>file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>housing</td>\n",
       "      <td>linear_regression</td>\n",
       "      <td>107927.898221</td>\n",
       "      <td>78010.955617</td>\n",
       "      <td>0.489922</td>\n",
       "      <td>backend/app/models/housing_linear.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>housing</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>104415.871731</td>\n",
       "      <td>74723.322464</td>\n",
       "      <td>0.522578</td>\n",
       "      <td>backend/app/models/housing_xgb.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>electricity</td>\n",
       "      <td>linear_regression</td>\n",
       "      <td>6053.178985</td>\n",
       "      <td>4916.777298</td>\n",
       "      <td>0.494813</td>\n",
       "      <td>backend/app/models/electricity_linear.joblib</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>electricity</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>1065.067224</td>\n",
       "      <td>814.119274</td>\n",
       "      <td>0.984360</td>\n",
       "      <td>backend/app/models/electricity_xgb.joblib</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       dataset         model_type           rmse           mae        r2  \\\n",
       "0      housing  linear_regression  107927.898221  78010.955617  0.489922   \n",
       "1      housing            xgboost  104415.871731  74723.322464  0.522578   \n",
       "2  electricity  linear_regression    6053.178985   4916.777298  0.494813   \n",
       "3  electricity            xgboost    1065.067224    814.119274  0.984360   \n",
       "\n",
       "                                           file  \n",
       "0      backend/app/models/housing_linear.joblib  \n",
       "1         backend/app/models/housing_xgb.joblib  \n",
       "2  backend/app/models/electricity_linear.joblib  \n",
       "3     backend/app/models/electricity_xgb.joblib  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if results:\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    display(metrics_df)\n",
    "else:\n",
    "    print(\"No results â€“ check training cell.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cd0104d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading housing XGB bundle from: backend/app/models/housing_xgb.joblib\n",
      "Model type: xgboost\n",
      "Features expected by the model: ['region', 'property_type', 'tenure', 'year', 'month', 'is_new_build']\n",
      "Target: price\n",
      "\n",
      "Downloading housing parquet again for defaults...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to parse string \"NORFOLK\" at position 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to parse string \"NORFOLK\"",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27013/2290553027.py\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrequired_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mfeature_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/tools/numeric.py\u001b[0m in \u001b[0;36mto_numeric\u001b[0;34m(arg, errors, downcast, dtype_backend)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mcoerce_numeric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m             values, new_mask = lib.maybe_convert_numeric(  # type: ignore[call-overload]\n\u001b[0m\u001b[1;32m    236\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                 \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.maybe_convert_numeric\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Unable to parse string \"NORFOLK\" at position 0"
     ]
    }
   ],
   "source": [
    "# Interactive prediction for a single house using the housing XGB model\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype, is_bool_dtype\n",
    "from IPython.display import display\n",
    "\n",
    "# 1) Load housing XGB bundle from backend/app/models\n",
    "housing_xgb_path = os.path.join(MODEL_DIR, \"housing_xgb.joblib\")\n",
    "print(\"Loading housing XGB bundle from:\", housing_xgb_path)\n",
    "housing_bundle = joblib.load(housing_xgb_path)\n",
    "\n",
    "model = housing_bundle[\"model\"]\n",
    "feature_names = housing_bundle[\"features\"]\n",
    "target_col = housing_bundle[\"target\"]\n",
    "\n",
    "print(\"Model type:\", housing_bundle.get(\"model_type\"))\n",
    "print(\"Features expected by the model:\", feature_names)\n",
    "print(\"Target:\", target_col)\n",
    "\n",
    "# 2) Reload housing dataset to compute sensible defaults\n",
    "housing_cfg = [d for d in datasets if d[\"name\"] == \"housing\"][0]\n",
    "local_parquet = \"/tmp/housing_for_prediction.parquet\"\n",
    "print(\"\\nDownloading housing parquet again for defaults...\")\n",
    "s3.download_file(BUCKET_NAME, housing_cfg[\"s3_key\"], local_parquet)\n",
    "df = pd.read_parquet(local_parquet)\n",
    "df = clean_nullable_dtypes(df)\n",
    "\n",
    "# Make sure 'tenure' exists (from 'duration' if needed)\n",
    "if \"tenure\" not in df.columns and \"duration\" in df.columns:\n",
    "    df = df.rename(columns={\"duration\": \"tenure\"})\n",
    "\n",
    "required_cols = [\"region\", \"property_type\", \"tenure\", \"year\", \"month\", \"is_new_build\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Housing parquet is missing required columns {missing} for interactive defaults.\")\n",
    "\n",
    "# Only numeric columns need casting; categoricals stay as strings/ints\n",
    "numeric_cols = [\"year\", \"month\"]\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "for col in required_cols:\n",
    "    df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "feature_df = df[feature_names].copy()\n",
    "\n",
    "print(\"\\nSample of training features:\")\n",
    "display(feature_df.head())\n",
    "\n",
    "# 3) Ask the user for input values for each feature\n",
    "print(\"\\n=== Interactive input for a single house ===\")\n",
    "print(\"For each feature, you'll see:\")\n",
    "print(\" - Expected or possible values\")\n",
    "print(\" - A default (from training data)\")\n",
    "print(\"Press ENTER to accept the default.\\n\")\n",
    "\n",
    "row = {}\n",
    "\n",
    "for col in feature_names:\n",
    "    series = feature_df[col].dropna()\n",
    "\n",
    "    if series.empty:\n",
    "        default = 0.0\n",
    "        expected_info = \"No data available, defaulting to 0.0.\"\n",
    "    else:\n",
    "        if is_numeric_dtype(series):\n",
    "            default = float(series.median())\n",
    "        elif is_bool_dtype(series):\n",
    "            default = bool(series.mode().iloc[0])\n",
    "        else:\n",
    "            default = str(series.mode().iloc[0])\n",
    "\n",
    "        unique_vals = np.unique(series.values)\n",
    "        if len(unique_vals) <= 20:\n",
    "            expected_info = f\"Expected values: {list(unique_vals)}\"\n",
    "        else:\n",
    "            if is_numeric_dtype(series):\n",
    "                expected_info = (\n",
    "                    f\"Expected numeric range: approx [{series.min():.3g}, {series.max():.3g}] \"\n",
    "                    f\"(median={series.median():.3g})\"\n",
    "                )\n",
    "            else:\n",
    "                top_vals = series.value_counts().head(10).index.tolist()\n",
    "                expected_info = f\"Most common categories: {top_vals} (total unique={len(unique_vals)})\"\n",
    "\n",
    "    print(f\"\\nFeature: '{col}'\")\n",
    "    print(\" \", expected_info)\n",
    "    print(\"  Default:\", default)\n",
    "\n",
    "    user_in = input(f\"Enter value for '{col}' (press ENTER for default): \").strip()\n",
    "\n",
    "    if user_in == \"\":\n",
    "        value = default\n",
    "    else:\n",
    "        if is_numeric_dtype(series):\n",
    "            # decide int vs float\n",
    "            kind = series.dtype.kind\n",
    "            if kind in [\"i\", \"u\"]:\n",
    "                try:\n",
    "                    value = int(float(user_in))\n",
    "                except ValueError:\n",
    "                    print(f\"  Could not parse integer, using default for '{col}'.\")\n",
    "                    value = default\n",
    "            else:\n",
    "                try:\n",
    "                    value = float(user_in)\n",
    "                except ValueError:\n",
    "                    print(f\"  Could not parse number, using default for '{col}'.\")\n",
    "                    value = default\n",
    "        elif is_bool_dtype(series):\n",
    "            value = user_in.lower() in [\"1\", \"true\", \"yes\", \"y\", \"t\"]\n",
    "        else:\n",
    "            value = user_in\n",
    "\n",
    "    row[col] = value\n",
    "\n",
    "# 4) Build DataFrame and predict\n",
    "input_df = pd.DataFrame([row])[feature_names]\n",
    "print(\"\\n=== You entered the following values ===\")\n",
    "display(input_df)\n",
    "\n",
    "pred = model.predict(input_df)[0]\n",
    "print(f\"\\nEstimated {target_col} for this house: {pred:,.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
