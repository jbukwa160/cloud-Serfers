{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49671207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this ONLY if xgboost is not installed in your environment.\n",
    "# After running, restart the kernel once.\n",
    "!conda install -y -c conda-forge xgboost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7b062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# CONFIGURATION – EDIT THIS PART\n",
    "# ==============================\n",
    "\n",
    "# Your S3 bucket\n",
    "BUCKET_NAME = \"cloudserfers\"   # make sure there is NO trailing space\n",
    "\n",
    "# Exact S3 keys of your parquet files inside the bucket\n",
    "# Example:\n",
    "#   s3://cloudserfers/uk_housing.parquet\n",
    "#   s3://cloudserfers/uk_electricity.parquet\n",
    "HOUSING_KEY     = \"uk_housing.parquet\"       # <-- change to your actual housing key\n",
    "ELECTRICITY_KEY = \"uk_electricity.parquet\"   # <-- change to your actual electricity key\n",
    "\n",
    "# Reproducibility + test size\n",
    "RANDOM_SEED = 123\n",
    "TEST_SIZE   = 0.2  # 20% test set\n",
    "\n",
    "# Datasets configuration\n",
    "# From your earlier error we know housing has:\n",
    "# ['transaction_unique_identifier', 'price', 'date_of_transfer', 'property_type',\n",
    "#  'oldnew', 'duration', 'towncity', 'district', 'county', 'ppdcategory_type',\n",
    "#  'record_status__monthly_file_only', 'year', 'month', 'region', 'is_new_build']\n",
    "# and electricity has ['ts', 'demand_mw'].\n",
    "\n",
    "datasets = [\n",
    "    {\n",
    "        \"name\": \"housing\",\n",
    "        \"s3_key\": HOUSING_KEY,\n",
    "        \"target\": \"price\",   # housing price column\n",
    "        \"drop_cols\": [\n",
    "            # Optional. Non-numerics will be dropped anyway, so we can leave this empty\n",
    "            # \"transaction_unique_identifier\"\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"electricity\",\n",
    "        \"s3_key\": ELECTRICITY_KEY,\n",
    "        \"target\": \"demand_mw\",\n",
    "        \"drop_cols\": [\n",
    "            # nothing needed here\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "print(\"Bucket:\", BUCKET_NAME)\n",
    "print(\"Configured datasets:\")\n",
    "for ds in datasets:\n",
    "    print(f\" - {ds['name']}: s3://{BUCKET_NAME}/{ds['s3_key']} (target='{ds['target']}')\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bfe3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.api.types import is_string_dtype, is_bool_dtype\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "print(\"Imports OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f05803",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nullable_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert pandas nullable dtypes into normal dtypes and replace pd.NA with np.nan.\n",
    "    This helps avoid weird dtype issues when training.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert pandas StringDtype to object\n",
    "    for col in df.columns:\n",
    "        if is_string_dtype(df[col]):\n",
    "            df[col] = df[col].astype(\"object\")\n",
    "    \n",
    "    # Convert nullable bool to float\n",
    "    for col in df.columns:\n",
    "        if is_bool_dtype(df[col]):\n",
    "            df[col] = df[col].astype(\"float64\")\n",
    "    \n",
    "    # Replace pandas NA with numpy nan\n",
    "    df = df.replace({pd.NA: np.nan})\n",
    "    return df\n",
    "\n",
    "print(\"clean_nullable_dtypes() is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbacc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "results = []\n",
    "\n",
    "for ds in datasets:\n",
    "    name      = ds[\"name\"]\n",
    "    key       = ds[\"s3_key\"]\n",
    "    target    = ds[\"target\"]\n",
    "    drop_cols = ds.get(\"drop_cols\", [])\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"S3 path: s3://{BUCKET_NAME}/{key}\")\n",
    "    print(f\"Target : {target}\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    # 1) Download parquet from S3 to the notebook instance\n",
    "    local_parquet = f\"/tmp/{name}.parquet\"\n",
    "    print(\"Downloading parquet from S3...\")\n",
    "    s3.download_file(BUCKET_NAME, key, local_parquet)\n",
    "    print(\"Downloaded to:\", local_parquet)\n",
    "\n",
    "    # 2) Read parquet\n",
    "    print(\"Reading parquet into pandas...\")\n",
    "    df = pd.read_parquet(local_parquet)\n",
    "    print(\"Raw shape:\", df.shape)\n",
    "    print(\"Raw dtypes:\\n\", df.dtypes)\n",
    "\n",
    "    # 3) Clean nullable dtypes\n",
    "    df = clean_nullable_dtypes(df)\n",
    "    print(\"Dtypes after cleaning:\\n\", df.dtypes)\n",
    "\n",
    "    # 3b) Dataset-specific feature engineering\n",
    "    # ----------------------------------------\n",
    "    if name == \"housing\":\n",
    "        # Your housing dataset already has 'year' and 'month' columns.\n",
    "        # Just make sure they are numeric and use them as features.\n",
    "        if \"year\" not in df.columns or \"month\" not in df.columns:\n",
    "            raise ValueError(\n",
    "                f\"Housing dataset: expected 'year' and 'month' columns, \"\n",
    "                f\"found columns: {list(df.columns)}\"\n",
    "            )\n",
    "\n",
    "        df[\"year\"] = pd.to_numeric(df[\"year\"])\n",
    "        df[\"month\"] = pd.to_numeric(df[\"month\"])\n",
    "\n",
    "        print(\"Housing: using existing 'year' and 'month' columns as features.\")\n",
    "        print(df[[\"year\", \"month\", target]].head())\n",
    "\n",
    "    elif name == \"electricity\":\n",
    "        # Electricity: create time-based features from 'ts'\n",
    "        ts_candidates = [\"ts\", \"timestamp\", \"datetime\"]\n",
    "        ts_col = None\n",
    "        for c in ts_candidates:\n",
    "            if c in df.columns:\n",
    "                ts_col = c\n",
    "                break\n",
    "\n",
    "        if ts_col is None:\n",
    "            raise ValueError(\n",
    "                f\"Electricity dataset: expected a time column in {ts_candidates}, \"\n",
    "                f\"found columns: {list(df.columns)}\"\n",
    "            )\n",
    "\n",
    "        df[ts_col] = pd.to_datetime(df[ts_col])\n",
    "        df[\"year\"]      = df[ts_col].dt.year\n",
    "        df[\"month\"]     = df[ts_col].dt.month\n",
    "        df[\"dayofweek\"] = df[ts_col].dt.dayofweek\n",
    "        df[\"hour\"]      = df[ts_col].dt.hour\n",
    "\n",
    "        # Drop original time column (optional, non-numeric)\n",
    "        df = df.drop(columns=[ts_col])\n",
    "\n",
    "        print(f\"Electricity: created year/month/dayofweek/hour from '{ts_col}'.\")\n",
    "        print(df[[\"year\", \"month\", \"dayofweek\", \"hour\", target]].head())\n",
    "\n",
    "    # 4) Drop unwanted columns from config (optional)\n",
    "    drop_now = [c for c in drop_cols if c in df.columns]\n",
    "    if drop_now:\n",
    "        df = df.drop(columns=drop_now)\n",
    "        print(\"Dropped columns:\", drop_now)\n",
    "\n",
    "    # 5) Make sure target exists\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Target '{target}' not found in columns: {list(df.columns)}\")\n",
    "\n",
    "    # 6) Keep only numeric columns (simple approach)\n",
    "    numeric_df = df.select_dtypes(include=[\"number\"]).copy()\n",
    "    if target not in numeric_df.columns:\n",
    "        raise ValueError(\n",
    "            f\"After selecting numeric columns, target '{target}' is missing. \"\n",
    "            f\"Numeric columns: {list(numeric_df.columns)}\"\n",
    "        )\n",
    "\n",
    "    print(\"Numeric df shape:\", numeric_df.shape)\n",
    "\n",
    "    # 7) Build X and y\n",
    "    X = numeric_df.drop(columns=[target])\n",
    "    y = numeric_df[target]\n",
    "\n",
    "    print(\"Feature columns used for model:\", list(X.columns))\n",
    "    if X.shape[1] == 0:\n",
    "        raise ValueError(\n",
    "            f\"Dataset '{name}': no feature columns left after preprocessing. \"\n",
    "            f\"Check drop_cols and feature engineering.\"\n",
    "        )\n",
    "\n",
    "    # 8) Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    print(\"Train shape:\", X_train.shape)\n",
    "    print(\"Test shape :\", X_test.shape)\n",
    "\n",
    "    # Drop rows with NaNs in features if needed\n",
    "    train_nulls = X_train.isna().sum().sum()\n",
    "    test_nulls  = X_test.isna().sum().sum()\n",
    "    if train_nulls or test_nulls:\n",
    "        print(f\"Found NaNs in features (train={train_nulls}, test={test_nulls}). Dropping rows with NaNs.\")\n",
    "        train_mask = ~X_train.isna().any(axis=1)\n",
    "        test_mask  = ~X_test.isna().any(axis=1)\n",
    "        X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "        X_test, y_test   = X_test[test_mask], y_test[test_mask]\n",
    "        print(\"After dropping NaNs:\")\n",
    "        print(\"Train shape:\", X_train.shape)\n",
    "        print(\"Test shape :\", X_test.shape)\n",
    "\n",
    "    # 9) Base XGB model\n",
    "    base_xgb = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=RANDOM_SEED,\n",
    "        tree_method=\"hist\",     # faster on CPUs\n",
    "        eval_metric=\"rmse\",\n",
    "    )\n",
    "\n",
    "    # 10) Hyperparameter search space (small to avoid burning credits)\n",
    "    param_dist = {\n",
    "        \"n_estimators\": [200, 400, 600],\n",
    "        \"max_depth\": [4, 6, 8],\n",
    "        \"learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"subsample\": [0.7, 0.85, 1.0],\n",
    "        \"colsample_bytree\": [0.7, 0.85, 1.0],\n",
    "        \"reg_lambda\": [0.5, 1.0, 2.0],\n",
    "    }\n",
    "\n",
    "    # Use neg_mean_squared_error so it works with older sklearn\n",
    "    tuner = RandomizedSearchCV(\n",
    "        estimator=base_xgb,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,   # 10 x 3-fold = 30 fits per dataset\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    print(\"Starting hyperparameter search...\")\n",
    "    tuner.fit(X_train, y_train)\n",
    "    print(\"Best params:\", tuner.best_params_)\n",
    "\n",
    "    best_model = tuner.best_estimator_\n",
    "\n",
    "    # 11) Evaluate on test set\n",
    "    y_pred = best_model.predict(X_test)\n",
    "\n",
    "    mse  = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5      # RMSE\n",
    "    mae  = mean_absolute_error(y_test, y_pred)\n",
    "    r2   = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Test RMSE: {rmse:.3f}\")\n",
    "    print(f\"Test MAE : {mae:.3f}\")\n",
    "    print(f\"Test R²  : {r2:.3f}\")\n",
    "\n",
    "    # 12) Save model bundle (model + feature names + target) locally and upload to S3\n",
    "    feature_names = X.columns.tolist()\n",
    "    model_bundle = {\n",
    "        \"model\": best_model,\n",
    "        \"features\": feature_names,\n",
    "        \"target\": target,\n",
    "    }\n",
    "    model_filename = f\"xgb_tuned_{name}_bundle.joblib\"\n",
    "    joblib.dump(model_bundle, model_filename)\n",
    "    model_s3_key = f\"models/{model_filename}\"\n",
    "    s3.upload_file(model_filename, BUCKET_NAME, model_s3_key)\n",
    "    print(f\"Saved model bundle to s3://{BUCKET_NAME}/{model_s3_key}\")\n",
    "\n",
    "    results.append({\n",
    "        \"dataset\": name,\n",
    "        \"model_type\": \"XGBRegressor_tuned\",\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"r2\": r2,\n",
    "        \"best_params\": tuner.best_params_,\n",
    "        \"model_s3_uri\": f\"s3://{BUCKET_NAME}/{model_s3_key}\",\n",
    "    })\n",
    "\n",
    "print(\"\\nTraining loop finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "if results:\n",
    "    metrics_df = pd.DataFrame(results)\n",
    "    display(metrics_df)\n",
    "    metrics_df.to_csv(\"aws_xgb_metrics.csv\", index=False)\n",
    "    print(\"Saved metrics to aws_xgb_metrics.csv\")\n",
    "\n",
    "    try:\n",
    "        s3.upload_file(\"aws_xgb_metrics.csv\", BUCKET_NAME, \"models/aws_xgb_metrics.csv\")\n",
    "        print(f\"Uploaded metrics to s3://{BUCKET_NAME}/models/aws_xgb_metrics.csv\")\n",
    "    except Exception as e:\n",
    "        print(\"Could not upload metrics to S3 (optional):\", e)\n",
    "else:\n",
    "    print(\"No results – check training cell for errors.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12455054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export copies of both trained model bundles to a local folder\n",
    "# so you can download them from the Jupyter / SageMaker UI.\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "EXPORT_DIR = \"/home/ec2-user/SageMaker/model_exports\"\n",
    "os.makedirs(EXPORT_DIR, exist_ok=True)\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "for ds in datasets:\n",
    "    name = ds[\"name\"]\n",
    "    model_filename = f\"xgb_tuned_{name}_bundle.joblib\"\n",
    "    model_s3_key = f\"models/{model_filename}\"\n",
    "\n",
    "    print(f\"\\nDownloading {model_filename} from S3...\")\n",
    "    obj = s3.get_object(Bucket=BUCKET_NAME, Key=model_s3_key)\n",
    "\n",
    "    local_path = f\"{EXPORT_DIR}/{model_filename}\"\n",
    "    with open(local_path, \"wb\") as f:\n",
    "        f.write(obj[\"Body\"].read())\n",
    "\n",
    "    print(f\"Exported model saved at:\\n  {local_path}\")\n",
    "\n",
    "print(\"\\nAll model files exported.\")\n",
    "print(\"You can download them from the left sidebar in Jupyter → Files → model_exports/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37cd2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction for a single house using the trained XGBoost housing model\n",
    "\n",
    "import boto3\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype, is_bool_dtype\n",
    "from IPython.display import display\n",
    "\n",
    "print(\"=== Loading trained housing model bundle from S3 ===\")\n",
    "\n",
    "# 1) Find housing config from the global `datasets` list\n",
    "housing_cfg = [d for d in datasets if d.get(\"name\") == \"housing\"][0]\n",
    "housing_target = housing_cfg[\"target\"]\n",
    "housing_key = housing_cfg[\"s3_key\"]\n",
    "housing_drop_cols = housing_cfg.get(\"drop_cols\", [])\n",
    "\n",
    "# 2) Load model bundle (model + feature_names + target) from S3\n",
    "HOUSING_DATASET_NAME = \"housing\"  # must match the 'name' in datasets\n",
    "model_filename = f\"xgb_tuned_{HOUSING_DATASET_NAME}_bundle.joblib\"\n",
    "model_s3_key   = f\"models/{model_filename}\"\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "print(f\"Downloading model bundle from s3://{BUCKET_NAME}/{model_s3_key} ...\")\n",
    "obj = s3.get_object(Bucket=BUCKET_NAME, Key=model_s3_key)\n",
    "\n",
    "local_model_path = f\"/tmp/{model_filename}\"\n",
    "with open(local_model_path, \"wb\") as f:\n",
    "    f.write(obj[\"Body\"].read())\n",
    "\n",
    "bundle = joblib.load(local_model_path)\n",
    "model = bundle[\"model\"]\n",
    "feature_names = bundle[\"features\"]\n",
    "target_col = bundle[\"target\"]\n",
    "\n",
    "print(\"Model bundle loaded.\")\n",
    "print(\"Feature columns used by the model:\", feature_names)\n",
    "print(\"Target column:\", target_col)\n",
    "\n",
    "# 3) Reload housing dataset and preprocess it just like in training\n",
    "print(\"\\n=== Loading housing dataset for defaults & value ranges ===\")\n",
    "local_parquet = \"/tmp/housing_for_prediction.parquet\"\n",
    "s3.download_file(BUCKET_NAME, housing_key, local_parquet)\n",
    "df = pd.read_parquet(local_parquet)\n",
    "df = clean_nullable_dtypes(df)\n",
    "\n",
    "# Ensure 'year' and 'month' exist and are numeric (as in training)\n",
    "if \"year\" not in df.columns or \"month\" not in df.columns:\n",
    "    # fallback to derive from a date column if needed\n",
    "    date_candidates = [\"date_of_transfer\", \"date\", \"Date\", \"transfer_date\"]\n",
    "    date_col = None\n",
    "    for c in date_candidates:\n",
    "        if c in df.columns:\n",
    "            date_col = c\n",
    "            break\n",
    "    if date_col is None:\n",
    "        raise ValueError(\n",
    "            f\"Housing dataset: expected 'year'/'month' or a date column in {date_candidates}, \"\n",
    "            f\"found columns: {list(df.columns)}\"\n",
    "        )\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df[\"year\"] = df[date_col].dt.year\n",
    "    df[\"month\"] = df[date_col].dt.month\n",
    "\n",
    "df[\"year\"] = pd.to_numeric(df[\"year\"])\n",
    "df[\"month\"] = pd.to_numeric(df[\"month\"])\n",
    "\n",
    "# Drop any configured columns\n",
    "drop_now = [c for c in housing_drop_cols if c in df.columns]\n",
    "if drop_now:\n",
    "    df = df.drop(columns=drop_now)\n",
    "\n",
    "# Keep only numeric columns and align with model features\n",
    "numeric_df = df.select_dtypes(include=[\"number\"]).copy()\n",
    "if target_col not in numeric_df.columns:\n",
    "    raise ValueError(\n",
    "        f\"After filtering numeric columns, target '{target_col}' is missing. \"\n",
    "        f\"Numeric columns: {list(numeric_df.columns)}\"\n",
    "    )\n",
    "\n",
    "feature_df = numeric_df.drop(columns=[target_col])\n",
    "\n",
    "# Ensure we only use columns the model was trained on\n",
    "feature_df = feature_df[feature_names]\n",
    "\n",
    "print(\"Sample of numeric training features:\")\n",
    "display(feature_df.head())\n",
    "\n",
    "# 4) Interactively ask the user for each feature\n",
    "print(\"\\n=== Interactive input for a single house ===\")\n",
    "print(\"For each feature, you’ll see:\")\n",
    "print(\" - A suggested default value (from the training data)\")\n",
    "print(\" - The expected or possible values\")\n",
    "print(\"Press ENTER to accept the default.\\n\")\n",
    "\n",
    "row = {}\n",
    "\n",
    "for col in feature_names:\n",
    "    series = feature_df[col].dropna()\n",
    "\n",
    "    # If column somehow empty, just default to 0\n",
    "    if series.empty:\n",
    "        default = 0.0\n",
    "        expected_info = \"No data available (defaulting to 0.0).\"\n",
    "    else:\n",
    "        # Choose sensible default from training data\n",
    "        if is_numeric_dtype(series):\n",
    "            default = float(series.median())\n",
    "        elif is_bool_dtype(series):\n",
    "            default = bool(series.mode().iloc[0])\n",
    "        else:\n",
    "            default = str(series.mode().iloc[0])\n",
    "\n",
    "        # Build \"expected values\" info\n",
    "        unique_vals = np.unique(series.values)\n",
    "        if len(unique_vals) <= 20:\n",
    "            # List all possible values\n",
    "            expected_info = f\"Expected values: {list(unique_vals)}\"\n",
    "        else:\n",
    "            if is_numeric_dtype(series):\n",
    "                expected_info = (\n",
    "                    f\"Expected numeric range: approx [{series.min():.3g}, {series.max():.3g}] \"\n",
    "                    f\"(median={series.median():.3g})\"\n",
    "                )\n",
    "            else:\n",
    "                top_vals = series.value_counts().head(10).index.tolist()\n",
    "                expected_info = f\"Most common categories: {top_vals} (total unique={len(unique_vals)})\"\n",
    "\n",
    "    print(f\"\\nFeature: '{col}'\")\n",
    "    print(f\"  {expected_info}\")\n",
    "    print(f\"  Default: {default}\")\n",
    "\n",
    "    user_in = input(f\"Enter value for '{col}' (press ENTER for default): \").strip()\n",
    "\n",
    "    if user_in == \"\":\n",
    "        value = default\n",
    "    else:\n",
    "        # Try to cast to appropriate type\n",
    "        if is_numeric_dtype(series):\n",
    "            # Decide int vs float based on dtype kind\n",
    "            kind = series.dtype.kind\n",
    "            if kind in [\"i\", \"u\"]:  # integer types\n",
    "                try:\n",
    "                    value = int(float(user_in))\n",
    "                except ValueError:\n",
    "                    print(f\"  Could not parse integer, falling back to default for '{col}'.\")\n",
    "                    value = default\n",
    "            else:\n",
    "                try:\n",
    "                    value = float(user_in)\n",
    "                except ValueError:\n",
    "                    print(f\"  Could not parse number, falling back to default for '{col}'.\")\n",
    "                    value = default\n",
    "        elif is_bool_dtype(series):\n",
    "            value = user_in.lower() in [\"1\", \"true\", \"yes\", \"y\", \"t\"]\n",
    "        else:\n",
    "            value = user_in  # keep as string (shouldn't happen here; model uses numeric)\n",
    "\n",
    "    row[col] = value\n",
    "\n",
    "# 5) Build input DataFrame and predict\n",
    "input_df = pd.DataFrame([row])[feature_names]\n",
    "print(\"\\n=== You entered the following values ===\")\n",
    "display(input_df)\n",
    "\n",
    "pred = model.predict(input_df)[0]\n",
    "\n",
    "print(f\"\\nEstimated {target_col} for this house: {pred:,.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
