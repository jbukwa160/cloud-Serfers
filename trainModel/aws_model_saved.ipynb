{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdd85d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 - Install XGBoost (optional)\n",
    "!conda install -y -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ad61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 - Configuration\n",
    "BUCKET_NAME = \"cloud-project-cloutserfers\"\n",
    "HOUSING_KEY = \"part-0.parquet\"\n",
    "ELECTRICITY_KEY = \"electricity_all_cleaned.parquet\"\n",
    "MODEL_DIR = \"backend/app/models\"\n",
    "RANDOM_SEED = 123\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "datasets = [\n",
    "    {\"name\": \"housing\", \"s3_key\": HOUSING_KEY, \"target\": \"price\"},\n",
    "    {\"name\": \"electricity\", \"s3_key\": ELECTRICITY_KEY, \"target\": \"demand_mw\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb95dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.api.types import is_string_dtype, is_bool_dtype\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import joblib\n",
    "\n",
    "# Make sure model dir exists\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Imports OK; model dir:\", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca573ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nullable_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert pandas nullable dtypes into normal dtypes and replace pd.NA with np.nan.\n",
    "    This helps avoid weird dtype issues when training.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Convert pandas StringDtype to object\n",
    "    for col in df.columns:\n",
    "        if is_string_dtype(df[col]):\n",
    "            df[col] = df[col].astype(\"object\")\n",
    "    \n",
    "    # Convert nullable bool to float\n",
    "    for col in df.columns:\n",
    "        if is_bool_dtype(df[col]):\n",
    "            df[col] = df[col].astype(\"float64\")\n",
    "    \n",
    "    # Replace pandas NA with numpy nan\n",
    "    df = df.replace({pd.NA: np.nan})\n",
    "    return df\n",
    "\n",
    "print(\"clean_nullable_dtypes() is ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24f06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "results = []  # to store metrics for later display\n",
    "\n",
    "for ds in datasets:\n",
    "    name   = ds[\"name\"]\n",
    "    s3_key = ds[\"s3_key\"]\n",
    "    target = ds[\"target\"]\n",
    "\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"S3 path: s3://{BUCKET_NAME}/{s3_key}\")\n",
    "    print(f\"Target : {target}\")\n",
    "    print(\"==============================\\n\")\n",
    "\n",
    "    # 1) Download parquet from S3\n",
    "    local_parquet = f\"/tmp/{name}.parquet\"\n",
    "    print(\"Downloading parquet from S3...\")\n",
    "    s3.download_file(BUCKET_NAME, s3_key, local_parquet)\n",
    "    print(\"Downloaded to:\", local_parquet)\n",
    "\n",
    "    # 2) Read parquet\n",
    "    print(\"Reading parquet into pandas...\")\n",
    "    df = pd.read_parquet(local_parquet)\n",
    "    print(\"Raw shape:\", df.shape)\n",
    "    print(\"Raw columns:\", list(df.columns))\n",
    "\n",
    "    # 3) Clean nullable dtypes\n",
    "    df = clean_nullable_dtypes(df)\n",
    "\n",
    "    # 4) Dataset-specific feature engineering and fixed feature list\n",
    "    if name == \"housing\":\n",
    "        # We know housing has region/property_type/duration/year/month/is_new_build, etc.\n",
    "        # Rename duration -> tenure\n",
    "        if \"tenure\" not in df.columns and \"duration\" in df.columns:\n",
    "            df = df.rename(columns={\"duration\": \"tenure\"})\n",
    "\n",
    "        # Required raw input columns (as you specified)\n",
    "        feature_cols = [\"region\", \"property_type\", \"tenure\", \"year\", \"month\", \"is_new_build\"]\n",
    "        missing = [c for c in feature_cols if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Housing dataset is missing required columns: {missing}\")\n",
    "\n",
    "        # Define which are categorical vs numeric\n",
    "        cat_cols = [\"region\", \"property_type\", \"tenure\", \"is_new_build\"]\n",
    "        num_cols = [\"year\", \"month\"]\n",
    "\n",
    "        # Do NOT force categoricals to numeric; leave them as strings/ints\n",
    "        # Just make sure numeric ones are numeric:\n",
    "        for col in num_cols:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "        print(\"Housing feature columns:\", feature_cols)\n",
    "        print(df[feature_cols + [target]].head())\n",
    "\n",
    "    elif name == \"electricity\":\n",
    "        # We know electricity has 'ts' and 'demand_mw'\n",
    "        ts_candidates = [\"ts\", \"timestamp\", \"datetime\"]\n",
    "        ts_col = None\n",
    "        for c in ts_candidates:\n",
    "            if c in df.columns:\n",
    "                ts_col = c\n",
    "                break\n",
    "        if ts_col is None:\n",
    "            raise ValueError(\n",
    "                f\"Electricity dataset: expected one of {ts_candidates}, found {list(df.columns)}\"\n",
    "            )\n",
    "\n",
    "        df[ts_col] = pd.to_datetime(df[ts_col])\n",
    "\n",
    "        # Required raw inputs for electricity:\n",
    "        # year, month, day, hour, is_weekend\n",
    "        df[\"year\"]       = df[ts_col].dt.year\n",
    "        df[\"month\"]      = df[ts_col].dt.month\n",
    "        df[\"day\"]        = df[ts_col].dt.day\n",
    "        df[\"hour\"]       = df[ts_col].dt.hour\n",
    "        df[\"is_weekend\"] = (df[ts_col].dt.dayofweek >= 5).astype(int)\n",
    "\n",
    "        feature_cols = [\"year\", \"month\", \"day\", \"hour\", \"is_weekend\"]\n",
    "        cat_cols = []                      # all numeric here\n",
    "        num_cols = feature_cols[:]         # all numeric\n",
    "\n",
    "        for col in num_cols:\n",
    "            df[col] = pd.to_numeric(df[col])\n",
    "\n",
    "        print(\"Electricity feature columns:\", feature_cols)\n",
    "        print(df[feature_cols + [target]].head())\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset name: {name}\")\n",
    "\n",
    "    # 5) Ensure target exists and numeric\n",
    "    if target not in df.columns:\n",
    "        raise ValueError(f\"Target '{target}' not in columns: {list(df.columns)}\")\n",
    "    df[target] = pd.to_numeric(df[target])\n",
    "\n",
    "    # 6) Build final X, y (with fixed raw feature columns)\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target].copy()\n",
    "\n",
    "    print(\"Final X shape:\", X.shape)\n",
    "    print(\"Final y length:\", len(y))\n",
    "\n",
    "    # 7) Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # Drop rows with NaN\n",
    "    train_mask = ~X_train.isna().any(axis=1)\n",
    "    test_mask  = ~X_test.isna().any(axis=1)\n",
    "    X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "    X_test, y_test   = X_test[test_mask], y_test[test_mask]\n",
    "\n",
    "    print(\"Train shape:\", X_train.shape)\n",
    "    print(\"Test shape :\", X_test.shape)\n",
    "\n",
    "    # 8) Preprocessor: OneHot for categoricals, passthrough numerics\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "            (\"num\", \"passthrough\", num_cols),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # ---------- 9) Linear Regression pipeline ----------\n",
    "    lin_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", LinearRegression()),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    lin_pipeline.fit(X_train, y_train)\n",
    "    y_pred_lin = lin_pipeline.predict(X_test)\n",
    "\n",
    "    mse_lin  = mean_squared_error(y_test, y_pred_lin)\n",
    "    rmse_lin = mse_lin ** 0.5\n",
    "    mae_lin  = mean_absolute_error(y_test, y_pred_lin)\n",
    "    r2_lin   = r2_score(y_test, y_pred_lin)\n",
    "\n",
    "    print(f\"[{name}] Linear Regression - RMSE: {rmse_lin:.3f}, MAE: {mae_lin:.3f}, R2: {r2_lin:.3f}\")\n",
    "\n",
    "    lin_bundle = {\n",
    "        \"model\": lin_pipeline,        # pipeline with preprocessing + model\n",
    "        \"features\": feature_cols,     # RAW inputs backend must provide\n",
    "        \"target\": target,\n",
    "        \"model_type\": \"linear_regression\",\n",
    "    }\n",
    "\n",
    "    lin_filename = os.path.join(MODEL_DIR, f\"{name}_linear.joblib\")\n",
    "    joblib.dump(lin_bundle, lin_filename)\n",
    "    print(f\"Saved LinearRegression bundle to: {lin_filename}\")\n",
    "\n",
    "    results.append({\n",
    "        \"dataset\": name,\n",
    "        \"model_type\": \"linear_regression\",\n",
    "        \"rmse\": rmse_lin,\n",
    "        \"mae\": mae_lin,\n",
    "        \"r2\": r2_lin,\n",
    "        \"file\": lin_filename,\n",
    "    })\n",
    "\n",
    "    # ---------- 10) XGBoost pipeline ----------\n",
    "    xgb = XGBRegressor(\n",
    "        objective=\"reg:squarederror\",\n",
    "        random_state=RANDOM_SEED,\n",
    "        tree_method=\"hist\",\n",
    "        eval_metric=\"rmse\",\n",
    "    )\n",
    "\n",
    "    xgb_pipeline = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"model\", xgb),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    param_dist = {\n",
    "        \"model__n_estimators\": [200, 400, 600],\n",
    "        \"model__max_depth\": [4, 6, 8],\n",
    "        \"model__learning_rate\": [0.01, 0.05, 0.1],\n",
    "        \"model__subsample\": [0.7, 0.85, 1.0],\n",
    "        \"model__colsample_bytree\": [0.7, 0.85, 1.0],\n",
    "        \"model__reg_lambda\": [0.5, 1.0, 2.0],\n",
    "    }\n",
    "\n",
    "    tuner = RandomizedSearchCV(\n",
    "        estimator=xgb_pipeline,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=10,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        cv=3,\n",
    "        verbose=1,\n",
    "        random_state=RANDOM_SEED,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    print(f\"Starting XGBoost hyperparameter search for {name}...\")\n",
    "    tuner.fit(X_train, y_train)\n",
    "    print(\"Best params:\", tuner.best_params_)\n",
    "\n",
    "    best_xgb_pipeline = tuner.best_estimator_\n",
    "    y_pred_xgb = best_xgb_pipeline.predict(X_test)\n",
    "\n",
    "    mse_xgb  = mean_squared_error(y_test, y_pred_xgb)\n",
    "    rmse_xgb = mse_xgb ** 0.5\n",
    "    mae_xgb  = mean_absolute_error(y_test, y_pred_xgb)\n",
    "    r2_xgb   = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "    print(f\"[{name}] XGBRegressor - RMSE: {rmse_xgb:.3f}, MAE: {mae_xgb:.3f}, R2: {r2_xgb:.3f}\")\n",
    "\n",
    "    xgb_bundle = {\n",
    "        \"model\": best_xgb_pipeline,   # pipeline (preprocessor + XGB)\n",
    "        \"features\": feature_cols,     # RAW inputs backend must provide\n",
    "        \"target\": target,\n",
    "        \"model_type\": \"xgboost\",\n",
    "    }\n",
    "\n",
    "    xgb_filename = os.path.join(MODEL_DIR, f\"{name}_xgb.joblib\")\n",
    "    joblib.dump(xgb_bundle, xgb_filename)\n",
    "    print(f\"Saved XGBRegressor bundle to: {xgb_filename}\")\n",
    "\n",
    "    results.append({\n",
    "        \"dataset\": name,\n",
    "        \"model_type\": \"xgboost\",\n",
    "        \"rmse\": rmse_xgb,\n",
    "        \"mae\": mae_xgb,\n",
    "        \"r2\": r2_xgb,\n",
    "        \"file\": xgb_filename,\n",
    "    })\n",
    "\n",
    "print(\"\\nTraining finished for all datasets. All 4 models saved in:\", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1af1542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive prediction for a single house using the housing XGB model\n",
    "\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype, is_bool_dtype\n",
    "from IPython.display import display\n",
    "\n",
    "# 1) Load housing XGB bundle from backend/app/models\n",
    "housing_xgb_path = os.path.join(MODEL_DIR, \"housing_xgb.joblib\")\n",
    "print(\"Loading housing XGB bundle from:\", housing_xgb_path)\n",
    "housing_bundle = joblib.load(housing_xgb_path)\n",
    "\n",
    "model = housing_bundle[\"model\"]       # this is a Pipeline(preprocessor + XGB)\n",
    "feature_names = housing_bundle[\"features\"]  # ['region','property_type','tenure','year','month','is_new_build']\n",
    "target_col = housing_bundle[\"target\"]\n",
    "\n",
    "print(\"Model type:\", housing_bundle.get(\"model_type\"))\n",
    "print(\"Features expected by the model:\", feature_names)\n",
    "print(\"Target:\", target_col)\n",
    "\n",
    "# 2) Reload housing dataset to compute sensible defaults\n",
    "housing_cfg = [d for d in datasets if d[\"name\"] == \"housing\"][0]\n",
    "local_parquet = \"/tmp/housing_for_prediction.parquet\"\n",
    "print(\"\\nDownloading housing parquet again for defaults...\")\n",
    "s3.download_file(BUCKET_NAME, housing_cfg[\"s3_key\"], local_parquet)\n",
    "df = pd.read_parquet(local_parquet)\n",
    "df = clean_nullable_dtypes(df)\n",
    "\n",
    "# Make sure 'tenure' exists (from 'duration' if needed)\n",
    "if \"tenure\" not in df.columns and \"duration\" in df.columns:\n",
    "    df = df.rename(columns={\"duration\": \"tenure\"})\n",
    "\n",
    "required_cols = [\"region\", \"property_type\", \"tenure\", \"year\", \"month\", \"is_new_build\"]\n",
    "missing = [c for c in required_cols if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Housing parquet is missing required columns {missing} for interactive defaults.\")\n",
    "\n",
    "# Only numeric columns need to be numeric; leave categoricals as strings/ints\n",
    "numeric_cols = [\"year\", \"month\"]\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "feature_df = df[feature_names].copy()\n",
    "\n",
    "print(\"\\nSample of training features:\")\n",
    "display(feature_df.head())\n",
    "\n",
    "# 3) Ask the user for input values for each feature\n",
    "print(\"\\n=== Interactive input for a single house ===\")\n",
    "print(\"For each feature, you'll see:\")\n",
    "print(\" - Expected or possible values\")\n",
    "print(\" - A default (from training data)\")\n",
    "print(\"Press ENTER to accept the default.\\n\")\n",
    "\n",
    "row = {}\n",
    "\n",
    "for col in feature_names:\n",
    "    series = feature_df[col].dropna()\n",
    "\n",
    "    if series.empty:\n",
    "        default = 0.0\n",
    "        expected_info = \"No data available, defaulting to 0.0.\"\n",
    "    else:\n",
    "        if is_numeric_dtype(series):\n",
    "            default = float(series.median())\n",
    "        elif is_bool_dtype(series):\n",
    "            default = bool(series.mode().iloc[0])\n",
    "        else:\n",
    "            default = str(series.mode().iloc[0])\n",
    "\n",
    "        unique_vals = np.unique(series.values)\n",
    "        if len(unique_vals) <= 20:\n",
    "            expected_info = f\"Expected values: {list(unique_vals)}\"\n",
    "        else:\n",
    "            if is_numeric_dtype(series):\n",
    "                expected_info = (\n",
    "                    f\"Expected numeric range: approx [{series.min():.3g}, {series.max():.3g}] \"\n",
    "                    f\"(median={series.median():.3g})\"\n",
    "                )\n",
    "            else:\n",
    "                top_vals = series.value_counts().head(10).index.tolist()\n",
    "                expected_info = f\"Most common categories: {top_vals} (total unique={len(unique_vals)})\"\n",
    "\n",
    "    print(f\"\\nFeature: '{col}'\")\n",
    "    print(\" \", expected_info)\n",
    "    print(\"  Default:\", default)\n",
    "\n",
    "    user_in = input(f\"Enter value for '{col}' (press ENTER for default): \").strip()\n",
    "\n",
    "    if user_in == \"\":\n",
    "        value = default\n",
    "    else:\n",
    "        if is_numeric_dtype(series):\n",
    "            # decide int vs float\n",
    "            kind = series.dtype.kind\n",
    "            if kind in [\"i\", \"u\"]:\n",
    "                try:\n",
    "                    value = int(float(user_in))\n",
    "                except ValueError:\n",
    "                    print(f\"  Could not parse integer, using default for '{col}'.\")\n",
    "                    value = default\n",
    "            else:\n",
    "                try:\n",
    "                    value = float(user_in)\n",
    "                except ValueError:\n",
    "                    print(f\"  Could not parse number, using default for '{col}'.\")\n",
    "                    value = default\n",
    "        elif is_bool_dtype(series):\n",
    "            value = user_in.lower() in [\"1\", \"true\", \"yes\", \"y\", \"t\"]\n",
    "        else:\n",
    "            value = user_in\n",
    "\n",
    "    row[col] = value\n",
    "\n",
    "# 4) Build DataFrame and predict\n",
    "input_df = pd.DataFrame([row])[feature_names]\n",
    "print(\"\\n=== You entered the following values ===\")\n",
    "display(input_df)\n",
    "\n",
    "pred = model.predict(input_df)[0]\n",
    "print(f\"\\nEstimated {target_col} for this house: {pred:,.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
