{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "971ae8bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4 requests pandas --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f65923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.14.2)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (2.8)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install beautifulsoup4 requests pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8287a6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re, time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "BASE      = \"https://www.neso.energy\"\n",
    "LIST_TPL  = BASE + \"/data-portal/historic-demand-data?page={page}\"\n",
    "RAW_DIR   = Path(\"data/electricity/raw\")\n",
    "INT_DIR   = Path(\"data/electricity/intermediate\")\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (compatible; edu-project/1.0)\"}\n",
    "YEAR_MIN, YEAR_MAX = 2001, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f344e3f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_page(page:int):\n",
    "    \"\"\"Return list of dicts with year, href, filename from the table's download column.\"\"\"\n",
    "    url = LIST_TPL.format(page=page)\n",
    "    r = requests.get(url, headers=HEADERS, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        return []\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    rows = []\n",
    "    for tr in soup.select(\"table tbody tr\"):\n",
    "        title_td = tr.select_one(\"td.views-field-title\")\n",
    "        # the column you highlighted\n",
    "        download_td = tr.select_one(\"td.views-field-download\") or tr.find(\n",
    "            \"td\", class_=lambda c: c and \"views-field-download\" in c\n",
    "        )\n",
    "        if not download_td:\n",
    "            continue\n",
    "        a = download_td.find(\"a\", href=True)\n",
    "        if not a:\n",
    "            continue\n",
    "        href = a[\"href\"]\n",
    "        if href.startswith(\"/\"):\n",
    "            href = BASE + href\n",
    "        if \".csv\" not in href.lower():\n",
    "            continue\n",
    "\n",
    "        # pull year from title or href\n",
    "        text = (title_td.get_text(strip=True) if title_td else \"\") + \" \" + href\n",
    "        year = None\n",
    "        for y in re.findall(r\"\\b(20\\d{2}|200\\d)\\b\", text):\n",
    "            yi = int(y)\n",
    "            if YEAR_MIN <= yi <= YEAR_MAX:\n",
    "                year = yi\n",
    "                break\n",
    "        if year is None:\n",
    "            continue\n",
    "\n",
    "        m = re.search(r\"([^/]+\\.csv)(?:\\?.*)?$\", href)\n",
    "        filename = m.group(1) if m else f\"neso_{year}.csv\"\n",
    "        rows.append({\"year\": year, \"href\": href, \"filename\": filename})\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b40a3f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25 CSVs across pages.\n"
     ]
    }
   ],
   "source": [
    "seen = set()\n",
    "links = []\n",
    "empty_streak = 0\n",
    "MAX_PAGES = 200  # safety cap\n",
    "\n",
    "for page in range(MAX_PAGES):\n",
    "    page_rows = parse_page(page)\n",
    "    new_count = 0\n",
    "    for r in page_rows:\n",
    "        key = (r[\"year\"], r[\"href\"])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            links.append(r)\n",
    "            new_count += 1\n",
    "    if new_count == 0:\n",
    "        empty_streak += 1\n",
    "    else:\n",
    "        empty_streak = 0\n",
    "    # stop after a few empty pages in a row\n",
    "    if empty_streak >= 3:\n",
    "        break\n",
    "    time.sleep(0.2)  # be polite\n",
    "\n",
    "# keep only 2001–2025 and sort\n",
    "links = [r for r in links if YEAR_MIN <= r[\"year\"] <= YEAR_MAX]\n",
    "links.sort(key=lambda d: (d[\"year\"], d[\"filename\"]))\n",
    "print(f\"Found {len(links)} CSVs across pages.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf3e028d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 25/25 [00:00<00:00, 12483.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw files stored in: data\\electricity\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for r in tqdm(links, desc=\"Downloading\"):\n",
    "    dest = RAW_DIR / f\"{r['year']}_{r['filename']}\"\n",
    "    if dest.exists() and dest.stat().st_size > 0:\n",
    "        continue\n",
    "    with requests.get(r[\"href\"], headers=HEADERS, timeout=120, stream=True) as resp:\n",
    "        resp.raise_for_status()\n",
    "        with open(dest, \"wb\") as f:\n",
    "            for chunk in resp.iter_content(1024 * 256):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "print(\"Raw files stored in:\", RAW_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e5ee117",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining: 100%|██████████| 25/25 [00:03<00:00,  7.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV: data\\electricity\\intermediate\\electricity_all.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = sorted(RAW_DIR.glob(\"*.csv\"))\n",
    "assert files, f\"No CSVs in {RAW_DIR}\"\n",
    "\n",
    "# union of columns\n",
    "all_cols = set()\n",
    "for f in files:\n",
    "    try:\n",
    "        cols = pd.read_csv(f, nrows=0).columns.tolist()\n",
    "        all_cols.update(cols)\n",
    "    except Exception as e:\n",
    "        print(f\"Header read error {f.name}: {e}\")\n",
    "all_cols = sorted(all_cols)\n",
    "\n",
    "combined_csv = INT_DIR / \"electricity_all.csv\"\n",
    "header_written = False\n",
    "for f in tqdm(files, desc=\"Combining\"):\n",
    "    try:\n",
    "        for chunk in pd.read_csv(f, chunksize=200_000, low_memory=False):\n",
    "            chunk = chunk.reindex(columns=all_cols)\n",
    "            chunk[\"source_file\"] = f.name\n",
    "            chunk.to_csv(combined_csv, mode=\"a\", index=False, header=not header_written)\n",
    "            header_written = True\n",
    "    except Exception as e:\n",
    "        print(f\"Error combining {f.name}: {e}\")\n",
    "\n",
    "print(\"Combined CSV:\", combined_csv)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
