{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6894f736",
   "metadata": {},
   "source": [
    "# 01_download (housing)\n",
    "**Authors**: <fill names> | **Owner**: <name> | **Reviewer**: <name> | **Date**: 2025-11-07\n",
    "\n",
    "**Purpose:** Download Kaggle UK housing data (England/Wales), standardize columns, and write partitioned Parquet.\n",
    "\n",
    "> Tip: Keep code blocks small. Explain choices (NA strategy, outlier handling, feature decisions) in Markdown right above the code that implements them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70bc9d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas kaggle --quiet\n",
    "!pip install pyarrow fastparquet --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16828b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyarrow in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (22.0.0)\n",
      "Requirement already satisfied: fastparquet in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2024.11.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastparquet) (2.3.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastparquet) (2.3.3)\n",
      "Requirement already satisfied: cramjam>=2.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastparquet) (2.11.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fastparquet) (2025.7.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from fastparquet) (24.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.5.0->fastparquet) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.5.0->fastparquet) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=1.5.0->fastparquet) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pyarrow fastparquet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c71b25",
   "metadata": {},
   "source": [
    "\n",
    "### Setup\n",
    "- Requires Kaggle API credentials (`~/.kaggle/kaggle.json`).\n",
    "- Adjust `KAGGLE_DATASET` and file names to match the dataset you're using.\n",
    "- This notebook **does not** commit data to git; it writes to `data/raw/housing/` and `data/intermediate/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7dbaca1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directories ready: price_paid_records intermediate_data\n",
      "Using file: price_paid_records\\price_paid_records.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Folder where your CSV is stored\n",
    "RAW_DIR = Path(\"price_paid_records\")\n",
    "INT_DIR = Path(\"intermediate_data\")\n",
    "\n",
    "# Create directories if they don't exist\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "INT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Directories ready:\", RAW_DIR, INT_DIR)\n",
    "\n",
    "# Now point to your actual CSV file\n",
    "CSV_PATH = RAW_DIR / \"price_paid_records.csv\"\n",
    "\n",
    "# Check if it exists\n",
    "assert CSV_PATH.exists(), f\"File not found: {CSV_PATH}\"\n",
    "print(\"Using file:\", CSV_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592271a3",
   "metadata": {},
   "source": [
    "### (Option A) Download with Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "183a8ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KAGGLE_DATASET = \"INSERT/SLUG\"   # e.g., \"hm-land-registry/uk-housing-prices-paid\"\n",
    "# !kaggle datasets download -d $KAGGLE_DATASET -p $RAW_DIR --unzip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c98ab0d",
   "metadata": {},
   "source": [
    "### (Option B) Proceed from an existing CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11d8096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using raw CSV: price_paid_records\\price_paid_records.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('price_paid_records/price_paid_records.csv')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Point to your CSV file (2GB+). If there are multiple files, loop over them.\n",
    "\n",
    "# If you already have a cleaned file somewhere else, you can point directly to it here:\n",
    "\n",
    "# Try to locate a CSV in the raw folder first\n",
    "csv_files = list(RAW_DIR.glob(\"*.csv\"))\n",
    "if csv_files:\n",
    "    CSV_PATH = csv_files[0]\n",
    "    print(f\"Using raw CSV: {CSV_PATH}\")\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        \"No CSV file found in ../data/raw/housing or ../cleanedDataStored. \"\n",
    "        \"Please add your dataset first.\"\n",
    "    )\n",
    "\n",
    "CSV_PATH\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5891e0d6",
   "metadata": {},
   "source": [
    "### Convert to partitioned Parquet (by year, region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbd54def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.str.strip()\n",
    "                  .str.lower()\n",
    "                  .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "                  .str.replace(r\"[^0-9a-zA-Z_]+\", \"\", regex=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def prepare_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    # Parse a date column if present (typical PPD column is 'date_of_transfer')\n",
    "    date_cols = [c for c in df.columns if \"date\" in c or \"transfer\" in c]\n",
    "    if date_cols:\n",
    "        dc = date_cols[0]\n",
    "        df[dc] = pd.to_datetime(df[dc], errors=\"coerce\")\n",
    "        df[\"year\"] = df[dc].dt.year\n",
    "        df[\"month\"] = df[dc].dt.month\n",
    "    else:\n",
    "        df[\"year\"] = pd.NA\n",
    "        df[\"month\"] = pd.NA\n",
    "\n",
    "    # Try to build a 'region' column if you have county/district/etc.\n",
    "    region_cols = [c for c in [\"county\",\"district\",\"local_authority\",\"region\",\"town_city\"] if c in df.columns]\n",
    "    if region_cols:\n",
    "        df[\"region\"] = df[region_cols[0]].astype(\"string\")\n",
    "    else:\n",
    "        df[\"region\"] = pd.Series(pd.Categorical([\"unknown\"] * len(df))).astype(\"string\")\n",
    "\n",
    "    # Ensure price is numeric if present\n",
    "    if \"price\" in df.columns:\n",
    "        df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54b880d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written ~5,000,000 rows to intermediate_data\\housing_all.csv\n",
      "Written ~10,000,000 rows to intermediate_data\\housing_all.csv\n",
      "Written ~15,000,000 rows to intermediate_data\\housing_all.csv\n",
      "Written ~20,000,000 rows to intermediate_data\\housing_all.csv\n",
      "Wrote: intermediate_data\\housing_all.csv (rows: ~21,550,210)\n",
      "Partitioned CSV files written to: intermediate_data\\partitioned_csv\n"
     ]
    }
   ],
   "source": [
    "out_csv = INT_DIR / \"housing_all.csv\"\n",
    "header_written = out_csv.exists() and out_csv.stat().st_size > 0\n",
    "row_count = 0\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(CSV_PATH, chunksize=1_000_000, low_memory=False), 1):\n",
    "    chunk = prepare_chunk(chunk)\n",
    "    # Append to CSV; write header only once\n",
    "    chunk.to_csv(out_csv, mode=\"a\", index=False, header=not header_written)\n",
    "    header_written = True\n",
    "    row_count += len(chunk)\n",
    "    if i % 5 == 0:\n",
    "        print(f\"Written ~{row_count:,} rows to {out_csv}\")\n",
    "\n",
    "print(f\"Wrote: {out_csv} (rows: ~{row_count:,})\")\n",
    "\n",
    "# ================================================\n",
    "# 2) OPTIONAL: WRITE PARTITIONED CSVs BY YEAR\n",
    "# ================================================\n",
    "part_dir = INT_DIR / \"partitioned_csv\"\n",
    "part_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Track which year files we've already created (to control headers)\n",
    "headers_written = set()\n",
    "\n",
    "for chunk in pd.read_csv(CSV_PATH, chunksize=1_000_000, low_memory=False):\n",
    "    chunk = prepare_chunk(chunk)\n",
    "    if \"year\" not in chunk.columns:\n",
    "        continue\n",
    "\n",
    "    # Drop rows without a year\n",
    "    chunk = chunk.dropna(subset=[\"year\"])\n",
    "    if chunk.empty:\n",
    "        continue\n",
    "\n",
    "    chunk[\"year\"] = chunk[\"year\"].astype(\"Int64\")\n",
    "\n",
    "    for yr, g in chunk.groupby(\"year\", sort=False):\n",
    "        out_path = part_dir / f\"year={int(yr)}.csv\"\n",
    "        write_header = int(yr) not in headers_written and not out_path.exists()\n",
    "        g.to_csv(out_path, mode=\"a\", index=False, header=write_header)\n",
    "        headers_written.add(int(yr))\n",
    "\n",
    "print(f\"Partitioned CSV files written to: {part_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
