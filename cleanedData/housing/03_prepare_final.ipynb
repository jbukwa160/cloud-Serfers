{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07886646",
   "metadata": {},
   "source": [
    "# 03_prepare_final (housing / CSV)\n",
    "**Authors**: <fill names> | **Owner**: <name> | **Reviewer**: <name> | **Date**: 2025-11-07\n",
    "\n",
    "**Purpose:** Deterministic pipeline CSV -> CSV\n",
    "\n",
    "> These notebooks assume the first ingest saved **CSV** outputs into `intermediate_data/` (not Parquet).\n",
    "> Paths used:\n",
    "> - Raw CSV: `price_paid_records/price_paid_records.csv`\n",
    "> - Combined CSV: `intermediate_data/housing_all.csv`\n",
    "> - Optional partitioned by year: `intermediate_data/partitioned_csv/year=YYYY.csv`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2fd91b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed4b40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\User\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow --quiet\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import shutil\n",
    "\n",
    "# Paths\n",
    "INT_DIR = Path(\"../../data/intermediate/housing\")\n",
    "PROC_DIR = Path(\"../../data/housing\")\n",
    "INT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROC_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "csv_path = INT_DIR / \"housing_all.csv\"\n",
    "assert csv_path.exists(), f\"Expected combined CSV at: {csv_path}\"\n",
    "\n",
    "# Outputs: ONE cleaned+standardized dataset in CSV + Parquet (dataset)\n",
    "clean_csv      = PROC_DIR / \"housing_cleaned_standardized.csv\"\n",
    "clean_parq_dir = PROC_DIR\n",
    "\n",
    "# Reset outputs\n",
    "if clean_csv.exists():\n",
    "    clean_csv.unlink()\n",
    "if clean_parq_dir.exists():\n",
    "    shutil.rmtree(clean_parq_dir)\n",
    "clean_parq_dir.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6896fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df.columns = (\n",
    "        df.columns.str.strip()\n",
    "                  .str.lower()\n",
    "                  .str.replace(r\"\\s+\", \"_\", regex=True)\n",
    "                  .str.replace(r\"[^0-9a-zA-Z_]+\", \"\", regex=True)\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def pipe_clean_chunk(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = normalize_columns(df).copy()\n",
    "\n",
    "    # price -> numeric\n",
    "    if \"price\" in df.columns:\n",
    "        df[\"price\"] = pd.to_numeric(df[\"price\"], errors=\"coerce\")\n",
    "\n",
    "    # region-like columns -> keep as string\n",
    "    for c in [\"region\", \"property_type\", \"tenure\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(\"string\")\n",
    "\n",
    "    # year / month -> numeric (we'll cast to float later)\n",
    "    for c in [\"year\", \"month\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "\n",
    "    # Chunk-level NA handling + outlier clipping for price\n",
    "    if \"price\" in df.columns:\n",
    "        median_price = df[\"price\"].median()\n",
    "        df[\"price\"] = df[\"price\"].fillna(median_price)\n",
    "\n",
    "        q1, q3 = df[\"price\"].quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lo, hi = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "        df[\"price\"] = df[\"price\"].clip(lo, hi)\n",
    "\n",
    "    # simple new-build flag\n",
    "    df[\"is_new_build\"] = False\n",
    "    for c in df.columns:\n",
    "        # only treat actual text columns as text\n",
    "        if \"new\" in c and df[c].dtype in (\"object\", \"string\"):\n",
    "            df[\"is_new_build\"] = df[c].astype(\"string\").str.contains(\"new\", case=False, na=False)\n",
    "            break\n",
    "\n",
    "    # drop useless index-ish columns if present\n",
    "    for c in [\"unnamed_0\", \"index\"]:\n",
    "        if c in df.columns:\n",
    "            df = df.drop(columns=c)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01a24e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10 chunks | clean rows so far: 2,000,000\n",
      "Processed 20 chunks | clean rows so far: 4,000,000\n",
      "Processed 30 chunks | clean rows so far: 6,000,000\n",
      "Processed 40 chunks | clean rows so far: 8,000,000\n",
      "Processed 50 chunks | clean rows so far: 10,000,000\n",
      "Processed 60 chunks | clean rows so far: 12,000,000\n",
      "Processed 70 chunks | clean rows so far: 14,000,000\n",
      "Processed 80 chunks | clean rows so far: 16,000,000\n",
      "Processed 90 chunks | clean rows so far: 18,000,000\n",
      "Processed 100 chunks | clean rows so far: 20,000,000\n",
      "Processed 110 chunks | clean rows so far: 22,000,000\n",
      "Processed 120 chunks | clean rows so far: 24,000,000\n",
      "Processed 130 chunks | clean rows so far: 26,000,000\n",
      "Processed 140 chunks | clean rows so far: 28,000,000\n",
      "Processed 150 chunks | clean rows so far: 30,000,000\n",
      "Processed 160 chunks | clean rows so far: 32,000,000\n",
      "Processed 170 chunks | clean rows so far: 34,000,000\n",
      "Processed 180 chunks | clean rows so far: 36,000,000\n",
      "Processed 190 chunks | clean rows so far: 38,000,000\n",
      "Processed 200 chunks | clean rows so far: 40,000,000\n",
      "Processed 210 chunks | clean rows so far: 42,000,000\n",
      "Done.\n",
      "Cleaned & standardized CSV -> ..\\..\\data\\housing\\housing_cleaned_standardized.csv\n",
      "Cleaned & standardized Parquet dataset dir -> ..\\..\\data\\housing\n"
     ]
    }
   ],
   "source": [
    "clean_header_written = False\n",
    "total_clean_rows = 0\n",
    "\n",
    "for i, chunk in enumerate(pd.read_csv(csv_path, chunksize=200_000, low_memory=False), start=1):\n",
    "    # 1) Clean the chunk\n",
    "    chunk_clean = pipe_clean_chunk(chunk)\n",
    "\n",
    "    # 2) STANDARDIZE DTYPES\n",
    "    from pandas.api.types import is_numeric_dtype\n",
    "    for col in chunk_clean.columns:\n",
    "        if is_numeric_dtype(chunk_clean[col]):\n",
    "            chunk_clean[col] = chunk_clean[col].astype(\"float64\")\n",
    "        else:\n",
    "            chunk_clean[col] = chunk_clean[col].astype(\"string\")\n",
    "\n",
    "    # 3) Append to cleaned CSV (full dataset)\n",
    "    chunk_clean.to_csv(\n",
    "        clean_csv,\n",
    "        mode=\"a\",\n",
    "        index=False,\n",
    "        header=not clean_header_written,\n",
    "    )\n",
    "    clean_header_written = True\n",
    "    total_clean_rows += len(chunk_clean)\n",
    "\n",
    "    # 4) Append to Parquet *dataset* (one file per chunk)\n",
    "    table_clean = pa.Table.from_pandas(chunk_clean, preserve_index=False)\n",
    "    pq.write_to_dataset(\n",
    "        table_clean,\n",
    "        root_path=clean_parq_dir,\n",
    "        basename_template=\"part-{i}.parquet\",  # <-- literal template, NOT f-string\n",
    "    )\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(f\"Processed {i} chunks | clean rows so far: {total_clean_rows:,}\")\n",
    "\n",
    "print(\"Done.\")\n",
    "print(\"Cleaned & standardized CSV ->\", clean_csv)\n",
    "print(\"Cleaned & standardized Parquet dataset dir ->\", clean_parq_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
